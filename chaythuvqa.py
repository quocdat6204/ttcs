# -*- coding: utf-8 -*-
"""ChaythuVQA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SB-NCo7UuJKRy-RhR0mKclinD3tKyEWR
"""

# !pip install torch torchvision transformers pillow albumentations opencv-python

import os
import torch
import torch.nn as nn
import numpy as np
from PIL import Image
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer,
    AutoModel,
    AutoImageProcessor,
    ViTModel,
    MBartForConditionalGeneration,
    BartConfig,
    Trainer,
    TrainingArguments,
    AutoProcessor
)
from transformers.modeling_outputs import BaseModelOutput
import albumentations as A
import cv2
from collections import Counter
import matplotlib.pyplot as plt
import json
import random

class ImageEncoder(nn.Module):
    """Mã hóa hình ảnh sử dụng Vision Transformer"""
    def __init__(self, output_dim=768):
        super().__init__()
        self.model_name = "google/vit-base-patch16-224-in21k"
        try:
            print(f"Đang tải {self.model_name}...")
            self.processor = AutoImageProcessor.from_pretrained(self.model_name, use_fast=True)
            self.model = ViTModel.from_pretrained(self.model_name)
            print(f"Đã tải xong {self.model_name}")
        except Exception as e:
            print(f"Lỗi khi tải {self.model_name}: {str(e)}")
            raise

        if output_dim != 768:
            self.projection = nn.Linear(768, output_dim)
        else:
            self.projection = nn.Identity()

        for param in list(self.model.parameters())[:-4]:
            param.requires_grad = False

    def forward(self, images, device):
        if isinstance(images, list):
            inputs = self.processor(images, return_tensors="pt")
            inputs = {k: v.to(device) for k, v in inputs.items()}
        else:
            inputs = self.processor(images, return_tensors="pt")
            inputs = {k: v.to(device) for k, v in inputs.items()}

        outputs = self.model(**inputs)
        image_embeddings = outputs.last_hidden_state[:, 0, :]
        image_embeddings = self.projection(image_embeddings)

        return image_embeddings


class QuestionEncoder(nn.Module):
    """Mã hóa câu hỏi sử dụng PhoBERT"""
    def __init__(self, output_dim=768):
        super().__init__()
        self.model_name = "vinai/phobert-base-v2"
        try:
            print(f"Đang tải {self.model_name}...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModel.from_pretrained(self.model_name)
            print(f"Đã tải xong {self.model_name}")
        except Exception as e:
            print(f"Lỗi khi tải {self.model_name}: {str(e)}")
            raise

        self.lstm = nn.LSTM(
            input_size=self.model.config.hidden_size,
            hidden_size=output_dim,
            num_layers=1,
            batch_first=True,
            bidirectional=True
        )

        self.projection = nn.Linear(output_dim * 2, output_dim)

        # Đóng băng một phần mô hình PhoBERT
        for param in list(self.model.parameters())[:-2]:
            param.requires_grad = False

    def forward(self, questions, device):
        inputs = self.tokenizer(
            questions,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=50
        )
        inputs = {k: v.to(device) for k, v in inputs.items()}
        outputs = self.model(**inputs)
        lstm_output, (hidden, _) = self.lstm(outputs.last_hidden_state)
        hidden = torch.cat([hidden[0], hidden[1]], dim=1)
        question_embeddings = self.projection(hidden)

        return question_embeddings


class VQAModel(nn.Module):
    """Mô hình VQA kết hợp encoder hình ảnh, encoder câu hỏi và decoder câu trả lời"""
    def __init__(self, hidden_dim=768):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.model_name = "vinai/bartpho-syllable-base"
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        try:
            print(f"Đang tải {self.model_name}...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.bartpho = MBartForConditionalGeneration.from_pretrained(self.model_name)
            print(f"Đã tải xong {self.model_name}")
        except Exception as e:
            print(f"Lỗi khi tải {self.model_name}: {str(e)}")
            raise

        self.image_encoder = ImageEncoder(output_dim=hidden_dim)
        self.question_encoder = QuestionEncoder(output_dim=hidden_dim)

        self.fusion = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim)
        )

        self.cross_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=8,
            dropout=0.1
        )

        self.layer_norm = nn.LayerNorm(hidden_dim)
        bartpho_dim = self.bartpho.config.d_model
        if hidden_dim != bartpho_dim:
            self.adapter = nn.Linear(hidden_dim, bartpho_dim)
        else:
            self.adapter = nn.Identity()

    def encode_inputs(self, images, questions):
        """Mã hóa đầu vào thành các vector đặc trưng"""
        image_embeddings = self.image_encoder(images, self.device)
        question_embeddings = self.question_encoder(questions, self.device)
        combined = torch.cat([image_embeddings, question_embeddings], dim=1)
        fused = self.fusion(combined)
        batch_size = fused.size(0)
        seq_length = 8
        sequence = fused.unsqueeze(1).expand(-1, seq_length, -1)
        sequence_t = sequence.transpose(0, 1)
        attn_output, _ = self.cross_attention(sequence_t, sequence_t, sequence_t)
        attn_output = attn_output.transpose(0, 1)
        attn_output = self.layer_norm(attn_output + sequence)
        attn_output = self.adapter(attn_output)

        return attn_output

    def forward(self, images, questions, labels=None):
        """Forward pass của mô hình"""
        encoder_hidden_states = self.encode_inputs(images, questions)
        encoder_outputs = BaseModelOutput(last_hidden_state=encoder_hidden_states)
        decoder_input_ids = None
        outputs = self.bartpho(
            encoder_outputs=encoder_outputs,
            decoder_input_ids=decoder_input_ids,
            labels=labels
        )

        return outputs

    def generate_answer(self, image, question, max_length=50, num_beams=4):
        """Sinh câu trả lời cho một cặp hình ảnh-câu hỏi"""
        device = next(self.parameters()).device
        self.eval()

        with torch.no_grad():
            images = [image] if not isinstance(image, list) else image
            questions = [question] if not isinstance(question, list) else question
            encoder_hidden_states = self.encode_inputs(images, questions)
            encoder_outputs = BaseModelOutput(last_hidden_state=encoder_hidden_states)
            outputs = self.bartpho.generate(
                encoder_outputs=encoder_outputs,
                max_length=max_length,
                num_beams=num_beams,
                early_stopping=True,
                temperature=1.0,
                do_sample=True,    # Sử dụng sampling
                top_p=0.9,         # Nucleus sampling
                no_repeat_ngram_size=2  # Tránh lặp lại n-gram
            )
            answer = self.tokenizer.decode(
                outputs[0],
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            ).strip()

            return answer

# Tạo instance của mô hình
model = VQAModel()

# Load checkpoint đầy đủ
checkpoint = torch.load(
    "checkpoint_epoch_15.pt",
    map_location=torch.device("cuda" if torch.cuda.is_available() else "cpu"),
    weights_only=False  # ⚠️ Rất quan trọng trong PyTorch >= 2.6
)

# Load weights cho model từ checkpoint
model.load_state_dict(checkpoint["model_state_dict"])

# Đưa model sang chế độ đánh giá và GPU (nếu có)
model.eval()
model.to(model.device)

def preprocess_image(image_path):
    """Đọc và tiền xử lý ảnh đầu vào phù hợp với AutoImageProcessor"""
    if isinstance(image_path, str):
        image = Image.open(image_path).convert("RGB")
    else:
        image = image_path
    return image

def vqa_infer(model, image_path, question):
    # Xử lý ảnh
    image_tensor = preprocess_image(image_path)

    # Chuyển về dạng batch (list) như model yêu cầu
    image_list = [image_tensor]
    question_list = [question]

    # Chạy sinh câu trả lời
    answer = model.generate_answer(image=image_list, question=question_list)
    return answer